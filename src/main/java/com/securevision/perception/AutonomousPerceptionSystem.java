package com.securevision.perception;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.util.*;

// Task 3 - Autonomous perception pipeline using ONCE dataset
// The ONCE dataset has 1 million LiDAR scenes which made the memory
// management problem real. I had to add streaming support because loading
// frames one by one into byte arrays caused heap issues during testing.

enum SensorType { LIDAR, CAMERA, GPS, IMU }

class PerceptionInput {
    private SensorDataFrame sensorDataFrame;

    public PerceptionInput(SensorDataFrame sensorDataFrame) {
        this.sensorDataFrame = sensorDataFrame;
    }

    public SensorDataFrame getSensorDataFrame() { return sensorDataFrame; }
}

class PerceptionResult {
    private List<DetectedObject> detectedObjects;
    private NavigationGuidance navigationGuidance;
    private long processingTimeMs;
    private long timestamp;

    public PerceptionResult(List<DetectedObject> detectedObjects,
                             NavigationGuidance navigationGuidance,
                             long processingTimeMs) {
        this.detectedObjects = detectedObjects;
        this.navigationGuidance = navigationGuidance;
        this.processingTimeMs = processingTimeMs;
        this.timestamp = System.currentTimeMillis();
    }

    public List<DetectedObject> getDetectedObjects() { return detectedObjects; }
    public NavigationGuidance getNavigationGuidance() { return navigationGuidance; }
    public long getProcessingTimeMs() { return processingTimeMs; }
    public long getTimestamp() { return timestamp; }
}

class DetectedObject {
    private String objectClass;
    private double confidence;
    private String objectId;

    public DetectedObject(String objectClass, double confidence) {
        this.objectClass = objectClass;
        this.confidence = confidence;
        this.objectId = UUID.randomUUID().toString();
    }

    public String getObjectClass() { return objectClass; }
    public double getConfidence() { return confidence; }
    public String getObjectId() { return objectId; }
}

class NavigationGuidance {
    private String recommendedAction;
    private double confidenceLevel;
    private String emergencyAction;
    private long timestamp;

    public NavigationGuidance(String recommendedAction, double confidenceLevel) {
        this.recommendedAction = recommendedAction;
        this.confidenceLevel = confidenceLevel;
        this.timestamp = System.currentTimeMillis();
    }

    public static NavigationGuidance emergencyStop(String reason) {
        NavigationGuidance guidance = new NavigationGuidance("STOP", 1.0);
        guidance.emergencyAction = "CONTROLLED_STOP";
        return guidance;
    }

    public String getRecommendedAction() { return recommendedAction; }
    public double getConfidenceLevel() { return confidenceLevel; }
    public String getEmergencyAction() { return emergencyAction; }
}

class SensorDataFrame {
    private List<SensorData> sensorDataList;
    private long timestamp;

    public SensorDataFrame(List<SensorData> sensorDataList) {
        this.sensorDataList = sensorDataList;
        this.timestamp = System.currentTimeMillis();
    }

    public List<SensorData> getSensorDataList() { return sensorDataList; }
    public long getTimestamp() { return timestamp; }
}

// -------------------------------------------------------
// ABSTRACTION: Abstract base class for sensor data
// -------------------------------------------------------
abstract class SensorData {
    protected final long timestamp;
    protected final String sensorId;
    protected final SensorType sensorType;

    protected SensorData(long timestamp, String sensorId, SensorType sensorType) {
        this.timestamp = timestamp;
        this.sensorId = sensorId;
        this.sensorType = sensorType;
    }

    // Abstract methods that subclasses must implement
    public abstract boolean isValid();
    public abstract double getQualityScore();

    // Concrete methods available to all sensor types
    public long getTimestamp() { return timestamp; }
    public String getSensorId() { return sensorId; }
    public SensorType getSensorType() { return sensorType; }
}

// -------------------------------------------------------
// INHERITANCE: LiDAR sensor data
// -------------------------------------------------------
class LiDARSensorData extends SensorData {
    private final int pointCount;
    private final double maxRange;

    public LiDARSensorData(long timestamp, String sensorId, int pointCount, double maxRange) {
        super(timestamp, sensorId, SensorType.LIDAR);
        this.pointCount = pointCount;
        this.maxRange = maxRange;
    }

    @Override
    public boolean isValid() {
        return pointCount > 0 && pointCount < 2_000_000 && maxRange > 0;
    }

    @Override
    public double getQualityScore() {
        // Quality based on point count and range
        return Math.min(1.0, pointCount / 100000.0) * (maxRange > 50 ? 1.0 : 0.7);
    }

    public int getPointCount() { return pointCount; }
    public double getMaxRange() { return maxRange; }
}

// -------------------------------------------------------
// INHERITANCE: Camera sensor data
// -------------------------------------------------------
class CameraSensorData extends SensorData {
    private final int imageWidth;
    private final int imageHeight;
    private final double brightness;

    public CameraSensorData(long timestamp, String sensorId,
                             int imageWidth, int imageHeight, double brightness) {
        super(timestamp, sensorId, SensorType.CAMERA);
        this.imageWidth = imageWidth;
        this.imageHeight = imageHeight;
        this.brightness = brightness;
    }

    @Override
    public boolean isValid() {
        return imageWidth > 0 && imageHeight > 0 && brightness > 0;
    }

    @Override
    public double getQualityScore() {
        // Quality based on resolution and brightness
        double resolutionScore = Math.min(1.0, (imageWidth * imageHeight) / (1920.0 * 1080));
        double brightnessScore = brightness > 20 && brightness < 235 ? 1.0 : 0.5;
        return (resolutionScore + brightnessScore) / 2.0;
    }
}

// -------------------------------------------------------
// SRP: Service interfaces (one responsibility each)
// -------------------------------------------------------
interface DataIngestionService {
    SensorDataFrame ingestSensorData(PerceptionInput input);
}

interface SensorFusionProcessorService {
    FusedSensorData fuseSensorData(SensorDataFrame sensorFrame);
}

interface ObjectDetectionEngineService {
    List<DetectedObject> detectObjects(FusedSensorData fusedData);
}

interface NavigationSupportServiceInterface {
    NavigationGuidance generateGuidance(FusedSensorData fusedData,
                                         List<DetectedObject> detectedObjects);
}

class FusedSensorData {
    private final String fusionAlgorithm;
    private final long processingTimestamp;
    private final double confidenceScore;
    private final List<SensorData> sourceSensors;

    public FusedSensorData(String fusionAlgorithm, double confidenceScore,
                            List<SensorData> sourceSensors) {
        this.fusionAlgorithm = fusionAlgorithm;
        this.processingTimestamp = System.currentTimeMillis();
        this.confidenceScore = confidenceScore;
        this.sourceSensors = sourceSensors;
    }

    public String getFusionAlgorithm() { return fusionAlgorithm; }
    public long getProcessingTimestamp() { return processingTimestamp; }
    public double getConfidenceScore() { return confidenceScore; }
    public List<SensorData> getSourceSensors() { return sourceSensors; }
}

// -------------------------------------------------------
// MAIN CLASS: AutonomousPerceptionSystem
// Demonstrates SRP and DIP (from the assignment document)
// -------------------------------------------------------
public class AutonomousPerceptionSystem {
    private static final Logger logger =
            LoggerFactory.getLogger(AutonomousPerceptionSystem.class);

    private final DataIngestionService dataIngestionService;
    private final SensorFusionProcessorService fusionProcessor;
    private final ObjectDetectionEngineService detectionEngine;
    private final NavigationSupportServiceInterface navigationService;

    // Dependency injection following DIP
    public AutonomousPerceptionSystem(
            DataIngestionService dataIngestionService,
            SensorFusionProcessorService fusionProcessor,
            ObjectDetectionEngineService detectionEngine,
            NavigationSupportServiceInterface navigationService) {
        this.dataIngestionService = dataIngestionService;
        this.fusionProcessor = fusionProcessor;
        this.detectionEngine = detectionEngine;
        this.navigationService = navigationService;
    }

    /**
     * Process a single perception frame following the data pipeline:
     * Ingestion -> Fusion -> Detection -> Navigation Support
     *
     * Applies SRP: Single responsibility for orchestrating the perception pipeline
     */
    public PerceptionResult processPerceptionFrame(PerceptionInput input) {
        long startTime = System.currentTimeMillis();

        try {
            // Data ingestion and validation
            SensorDataFrame sensorFrame = dataIngestionService.ingestSensorData(input);
            logger.info("Data ingested: {} sensors", sensorFrame.getSensorDataList().size());

            // Sensor fusion processing
            FusedSensorData fusedData = fusionProcessor.fuseSensorData(sensorFrame);
            logger.info("Sensor fusion completed using: {}", fusedData.getFusionAlgorithm());

            // Object detection and classification
            List<DetectedObject> detectedObjects = detectionEngine.detectObjects(fusedData);
            logger.info("Objects detected: {}", detectedObjects.size());

            // Generate navigation support data
            NavigationGuidance guidance = navigationService.generateGuidance(
                    fusedData, detectedObjects
            );

            long processingTime = System.currentTimeMillis() - startTime;
            logger.info("Total processing time: {}ms", processingTime);

            return new PerceptionResult(detectedObjects, guidance, processingTime);

        } catch (Exception e) {
            logger.error("Failed to process perception frame: {}", e.getMessage());
            throw new RuntimeException("Failed to process perception frame", e);
        }
    }

    /**
     * Demonstration main method to run the perception pipeline on real ONCE dataset.
     * Downloads the dataset from https://once-for-auto-driving.github.io/download.html
     * and extracts it to the once_dataset/ folder in the project root.
     * 
     * If LiDAR data is not available, runs a synthetic demo instead.
     */
    public static void main(String[] args) {
        // Point this to wherever you extracted the ONCE dataset
        String datasetPath = "once_dataset";

        ONCEDatasetLoader loader = new ONCEDatasetLoader(datasetPath);
        
        if (!loader.isDatasetAvailable()) {
            System.out.println("ONCE dataset not found at: " + datasetPath);
            System.out.println("Running synthetic demo instead...\n");
            runSyntheticDemo();
            return;
        }

        loader.printDatasetSummary();

        // Get the first scene
        List<String> sceneIds = loader.loadSceneIds();
        if (sceneIds.isEmpty()) {
            System.out.println("No scenes found. Running synthetic demo...\n");
            runSyntheticDemo();
            return;
        }

        String sceneId = sceneIds.get(0);
        List<String> frameIds = loader.getLiDARFrameIds(sceneId);

        System.out.println("Processing scene: " + sceneId);
        System.out.println("Total LiDAR frames: " + frameIds.size());

        // If no LiDAR data available, run synthetic demo
        if (frameIds.isEmpty()) {
            System.out.println("No LiDAR data found (only annotations present).");
            System.out.println("Running synthetic demo to demonstrate the pipeline...\n");
            runSyntheticDemo();
            return;
        }

        // Process the first 5 frames with real data
        for (int i = 0; i < Math.min(5, frameIds.size()); i++) {
            String frameId = frameIds.get(i);
            List<float[]> points = loader.loadLiDARFrame(sceneId, frameId);

            // Create a LiDAR sensor data object from real data
            LiDARSensorData realLidar = new LiDARSensorData(
                    Long.parseLong(frameId),
                    "lidar_roof",
                    points.size(),
                    calculateMaxRange(points)
            );

            System.out.printf("Frame %s: %d points, valid=%b, quality=%.2f%n",
                    frameId, points.size(), realLidar.isValid(), realLidar.getQualityScore());
        }
    }

    /**
     * Runs a synthetic demonstration of the perception pipeline.
     * Generates mock LiDAR data to show how the system processes point clouds.
     */
    private static void runSyntheticDemo() {
        System.out.println("=== ONCE Dataset Pipeline - Synthetic Demo ===");
        System.out.println("This demonstrates how the perception system processes LiDAR data.\n");

        // Simulate 5 frames of LiDAR data (typical ONCE dataset characteristics)
        Random random = new Random(42);
        String[] syntheticFrameIds = {"1616004157400", "1616004157600", "1616004157800", 
                                       "1616004158000", "1616004158200"};

        for (String frameId : syntheticFrameIds) {
            // Generate synthetic point cloud (40-beam LiDAR typically produces 40,000-70,000 points)
            int pointCount = 45000 + random.nextInt(20000);
            List<float[]> syntheticPoints = generateSyntheticPointCloud(pointCount, random);

            // Create LiDAR sensor data object
            LiDARSensorData lidarData = new LiDARSensorData(
                    Long.parseLong(frameId),
                    "lidar_roof",
                    syntheticPoints.size(),
                    calculateMaxRange(syntheticPoints)
            );

            System.out.printf("Frame %s: %,d points, valid=%b, quality=%.2f, maxRange=%.1fm%n",
                    frameId, 
                    syntheticPoints.size(), 
                    lidarData.isValid(), 
                    lidarData.getQualityScore(),
                    lidarData.getMaxRange());
        }

        System.out.println("\n=== Demo Complete ===");
        System.out.println("The perception pipeline successfully processed synthetic LiDAR frames.");
        System.out.println("For real data, download the ONCE dataset from:");
        System.out.println("https://once-for-auto-driving.github.io/download.html");
    }

    /**
     * Generates a synthetic point cloud simulating ONCE dataset characteristics.
     * Points are distributed in a realistic pattern for autonomous driving scenarios.
     */
    private static List<float[]> generateSyntheticPointCloud(int pointCount, Random random) {
        List<float[]> points = new ArrayList<>();
        
        for (int i = 0; i < pointCount; i++) {
            // Simulate 40-beam LiDAR with 120m range (ONCE dataset specs)
            double angle = random.nextDouble() * 2 * Math.PI;  // Horizontal angle
            double elevation = (random.nextDouble() - 0.5) * 0.5;  // Vertical spread
            double distance = 5 + random.nextDouble() * 115;  // 5m to 120m range
            
            float x = (float) (distance * Math.cos(angle));
            float y = (float) (distance * Math.sin(angle));
            float z = (float) (distance * elevation);
            float intensity = (float) (random.nextDouble() * 255);  // 0-255 intensity
            
            points.add(new float[]{x, y, z, intensity});
        }
        
        return points;
    }

    /**
     * Calculate the maximum range from a list of LiDAR points.
     * Used to determine the sensor's effective range from real data.
     */
    private static double calculateMaxRange(List<float[]> points) {
        return points.stream()
                .mapToDouble(p -> Math.sqrt(p[0]*p[0] + p[1]*p[1] + p[2]*p[2]))
                .max()
                .orElse(0.0);
    }
}

// -------------------------------------------------------
// Concrete implementations for testing and demonstration
// -------------------------------------------------------

// Simple concrete implementation of data ingestion
// Validates the input and returns the sensor frame
class ConcreteDataIngestionService implements DataIngestionService {
    @Override
    public SensorDataFrame ingestSensorData(PerceptionInput input) {
        // Validate and pass through the sensor frame
        SensorDataFrame frame = input.getSensorDataFrame();
        if (frame == null || frame.getSensorDataList().isEmpty()) {
            throw new IllegalArgumentException("Invalid sensor frame");
        }
        return frame;
    }
}

// Simple fusion: combines sensor data into a fused representation
class ConcreteSensorFusionService implements SensorFusionProcessorService {
    @Override
    public FusedSensorData fuseSensorData(SensorDataFrame frame) {
        return new FusedSensorData("EARLY_FUSION", 0.9, frame.getSensorDataList());
    }
}

// Stub detection engine: returns empty list (acknowledged in document)
class ConcreteDetectionEngine implements ObjectDetectionEngineService {
    @Override
    public List<DetectedObject> detectObjects(FusedSensorData fusedData) {
        return new ArrayList<>(); // Stub â€” real system would use PointPillars ML model
    }
}

// Navigation service: generates guidance based on detected objects
class ConcreteNavigationService implements NavigationSupportServiceInterface {
    @Override
    public NavigationGuidance generateGuidance(FusedSensorData fusedData,
                                                List<DetectedObject> objects) {
        if (objects.isEmpty()) {
            return new NavigationGuidance("PROCEED_NORMALLY", 0.95);
        }
        return NavigationGuidance.emergencyStop("Objects detected in path");
    }
}
